<!DOCTYPE html>
<html lang="ko">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OPT-350M íŒŒì¸íŠœë‹ ì™„ì „ ì •ë³µ (LoRA ì ìš©/ë¯¸ì ìš©)</title>
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- PrismJS for Syntax Highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />
    <!-- Google Fonts -->
    <link
        href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&family=Noto+Serif+KR:wght@400;700&family=JetBrains+Mono:wght@400;700&display=swap"
        rel="stylesheet">
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #f8f9fa;
            color: #333;
        }

        h1,
        h2,
        h3 {
            font-family: 'Noto Serif KR', serif;
        }

        code {
            font-family: 'JetBrains Mono', monospace;
        }

        /* Typography & Layout */
        .blog-content p {
            margin-bottom: 1.5rem;
            font-size: 1.125rem;
            line-height: 2rem;
            color: #374151;
        }

        .blog-content ul {
            list-style-type: disc;
            list-style-position: inside;
            margin-bottom: 1.5rem;
            margin-left: 1rem;
            color: #374151;
        }

        .blog-content li {
            margin-bottom: 0.5rem;
        }

        /* Table Styles */
        .blog-table {
            width: 100%;
            border-collapse: collapse;
            border-radius: 0.5rem;
            overflow: hidden;
            box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
            margin-bottom: 2rem;
            font-size: 0.875rem;
        }

        .blog-table th {
            background-color: #1e293b;
            color: white;
            font-weight: 600;
            padding: 1rem 1.5rem;
            text-align: left;
        }

        .blog-table td {
            border-bottom: 1px solid #e5e7eb;
            padding: 1rem 1.5rem;
            background-color: white;
        }

        .blog-table tr:last-child td {
            border-bottom: 0;
        }

        .blog-table tr:hover td {
            background-color: #eff6ff;
            transition: background-color 0.2s;
        }

        /* Code Block Styles */
        .code-wrapper {
            margin-bottom: 2rem;
            border-radius: 0.75rem;
            overflow: hidden;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
            border: 1px solid #e5e7eb;
        }

        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            background-color: #2d2d2d;
            color: #d1d5db;
            padding: 0.75rem 1.25rem;
            font-size: 0.875rem;
            border-bottom: 1px solid #4b5563;
        }

        pre[class*="language-"] {
            margin: 0 !important;
            border-radius: 0 !important;
            background-color: #1e1e1e !important;
        }

        /* Output Terminal Styles */
        .terminal-wrapper {
            margin-top: 0.5rem;
            margin-bottom: 2rem;
            background-color: black;
            border-radius: 0.5rem;
            box-shadow: inset 0 2px 4px 0 rgba(0, 0, 0, 0.06);
            overflow: hidden;
            border: 1px solid #1f2937;
            font-family: monospace;
            font-size: 0.875rem;
        }

        .terminal-header {
            display: flex;
            align-items: center;
            background-color: #111827;
            padding: 0.5rem 1rem;
            font-size: 0.75rem;
            color: #9ca3af;
            border-bottom: 1px solid #1f2937;
        }

        .terminal-dots {
            display: flex;
            gap: 0.375rem;
            margin-right: 0.75rem;
        }

        .dot {
            width: 0.75rem;
            height: 0.75rem;
            border-radius: 9999px;
        }

        .dot-red {
            background-color: #ff5f56;
        }

        .dot-yellow {
            background-color: #ffbd2e;
        }

        .dot-green {
            background-color: #27c93f;
        }

        .terminal-body {
            padding: 1rem;
            color: #4ade80;
            overflow-x: auto;
            white-space: pre;
        }

        .terminal-line {
            display: block;
            margin-bottom: 0.25rem;
        }

        .terminal-prompt {
            color: #60a5fa;
            margin-right: 0.5rem;
        }

        /* Custom Scrollbar */
        ::-webkit-scrollbar {
            width: 10px;
            height: 10px;
        }

        ::-webkit-scrollbar-track {
            background: #f1f1f1;
        }

        ::-webkit-scrollbar-thumb {
            background: #cbd5e1;
            border-radius: 5px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: #94a3b8;
        }

        /* Sidebar Active State */
        .nav-link.active {
            color: #2563eb;
            font-weight: 700;
            border-left: 4px solid #2563eb;
            padding-left: 0.75rem;
            background-color: #eff6ff;
        }

        .nav-link {
            padding-left: 1rem;
            padding-top: 0.5rem;
            padding-bottom: 0.5rem;
            display: block;
            border-left: 4px solid transparent;
            transition: all 0.2s;
        }

        /* Callout Box */
        .callout {
            background-color: #eff6ff;
            border-left: 4px solid #3b82f6;
            padding: 1rem;
            margin-bottom: 1.5rem;
            border-radius: 0 0.5rem 0.5rem 0;
        }

        .callout-title {
            font-weight: 700;
            color: #1e40af;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }
    </style>
</head>

<body>

    <!-- Header / Hero Section -->
    <header class="bg-gradient-to-r from-slate-900 to-slate-800 text-white pb-24 pt-10 px-4">
        <div class="container mx-auto max-w-6xl">
            <nav class="flex justify-between items-center mb-16">
                <div class="font-bold text-2xl tracking-tight flex items-center gap-2">
                    <i class="fas fa-layer-group text-blue-400"></i>
                    DevLog<span class="text-blue-400">.AI</span>
                </div>
                <div class="space-x-6 text-sm font-medium hidden md:block">
                    <a href="#" class="hover:text-blue-300 transition">Home</a>
                    <a href="#" class="hover:text-blue-300 transition">Tutorials</a>
                    <a href="#" class="hover:text-blue-300 transition">About</a>
                </div>
            </nav>
            <div class="max-w-3xl">
                <div class="inline-block bg-blue-600 text-xs font-bold px-2 py-1 rounded mb-4">LLM Fine-tuning</div>
                <h1 class="text-4xl md:text-5xl font-bold mb-6 leading-tight">
                    OPT-350M ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤ìŠµ:<br>
                    <span class="text-blue-300">LoRA ì ìš© vs ë¯¸ì ìš© ì™„ë²½ ë¹„êµ</span>
                </h1>
                <p class="text-lg text-gray-300 mb-8 max-w-2xl leading-relaxed">
                    Metaì˜ OPT-350M ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‚˜ë§Œì˜ ì»¤ìŠ¤í…€ ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ë‹ˆë‹¤.
                    ê¸°ë³¸ì ì¸ íŒŒì¸íŠœë‹ ë°©ë²•ë¶€í„° ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ LoRA(Low-Rank Adaptation) ê¸°ë²•ê¹Œì§€,
                    ì½”ë“œ í•œ ì¤„ í•œ ì¤„ ìƒì„¸í•˜ê²Œ ëœ¯ì–´ë³´ê³  ì‹¤í–‰ ê²°ê³¼ê¹Œì§€ í™•ì¸í•´ ë³´ì„¸ìš”.
                </p>
                <div class="flex items-center gap-4 text-sm text-gray-400">
                    <div class="flex items-center gap-2">
                        <img src="https://api.dicebear.com/7.x/avataaars/svg?seed=Felix" alt="Author"
                            class="w-8 h-8 rounded-full border border-gray-600">
                        <span>AI Researcher Kim</span>
                    </div>
                    <span>â€¢</span>
                    <span><i class="far fa-calendar-alt mr-1"></i> 2024. 05. 20</span>
                    <span>â€¢</span>
                    <span><i class="far fa-clock mr-1"></i> 15 min read</span>
                </div>
            </div>
        </div>
    </header>

    <div class="container mx-auto max-w-6xl px-4 -mt-16 flex flex-col lg:flex-row gap-8 pb-20">

        <!-- Sidebar (TOC) -->
        <aside class="lg:w-1/4 hidden lg:block">
            <div class="sticky top-10 bg-white rounded-xl shadow-lg border border-gray-200 overflow-hidden">
                <div class="bg-gray-50 px-6 py-4 border-b border-gray-100 font-bold text-gray-700">
                    ëª©ì°¨ (Table of Contents)
                </div>
                <nav class="py-4 text-sm text-gray-600">
                    <a href="#intro" class="nav-link hover:text-blue-500">1. ë“¤ì–´ê°€ë©°: OPT vs GPT-2</a>
                    <a href="#part1" class="nav-link hover:text-blue-500 mt-2 font-semibold text-gray-800">Part 1. LoRA
                        ë¯¸ì ìš© ì‹¤ìŠµ</a>
                    <div class="ml-2 border-l border-gray-200">
                        <a href="#part1-install" class="nav-link hover:text-blue-500">1-1. í™˜ê²½ ì„¤ì • ë° ì„¤ì¹˜</a>
                        <a href="#part1-model" class="nav-link hover:text-blue-500">1-2. ëª¨ë¸ ë¡œë“œ (Pad Token ì´ìŠˆ)</a>
                        <a href="#part1-data" class="nav-link hover:text-blue-500">1-3. ë°ì´í„°ì…‹ êµ¬ì„±</a>
                        <a href="#part1-train" class="nav-link hover:text-blue-500">1-4. í•™ìŠµ ì‹¤í–‰ (Training)</a>
                        <a href="#part1-result" class="nav-link hover:text-blue-500">1-5. ê²°ê³¼ í™•ì¸</a>
                    </div>
                    <a href="#part2" class="nav-link hover:text-blue-500 mt-2 font-semibold text-gray-800">Part 2. LoRA
                        ì ìš© ì‹¤ìŠµ</a>
                    <div class="ml-2 border-l border-gray-200">
                        <a href="#part2-setup" class="nav-link hover:text-blue-500">2-1. PEFT/LoRA ì„¤ì •</a>
                        <a href="#part2-train" class="nav-link hover:text-blue-500">2-2. íš¨ìœ¨ì ì¸ í•™ìŠµ ì‹¤í–‰</a>
                        <a href="#part2-result" class="nav-link hover:text-blue-500">2-3. ìµœì¢… ì¶”ë¡  ê²°ê³¼</a>
                    </div>
                </nav>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="lg:w-3/4 w-full bg-white rounded-xl shadow-xl border border-gray-200 p-8 md:p-12 blog-content">

            <!-- Intro Section -->
            <section id="intro" class="mb-16">
                <h2 class="text-3xl font-bold mb-6 text-slate-800 border-b pb-4">1. ë“¤ì–´ê°€ë©°: GPT-2 vs OPT-350M</h2>
                <p>
                    ì–¸ì–´ ëª¨ë¸ì„ ì²˜ìŒ ê³µë¶€í•  ë•Œ ê°€ì¥ ë§ì´ ì ‘í•˜ëŠ” ê²ƒì´ GPT-2ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ìµœê·¼ì—ëŠ” Metaì—ì„œ ê³µê°œí•œ
                    <strong>OPT (Open Pre-trained Transformer)</strong> ì‹œë¦¬ì¦ˆê°€ ì¢‹ì€ ëŒ€ì•ˆìœ¼ë¡œ ë– ì˜¤ë¥´ê³  ìˆìŠµë‹ˆë‹¤.
                    ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” OPT-350M ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ Q&A ì±—ë´‡ì„ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.
                </p>

                <div class="overflow-x-auto">
                    <table class="blog-table">
                        <thead>
                            <tr>
                                <th>í•­ëª©</th>
                                <th>GPT-2</th>
                                <th>OPT-350M</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="font-semibold text-gray-700">ê°œë°œì</td>
                                <td>OpenAI</td>
                                <td>Meta (Facebook AI Research)</td>
                            </tr>
                            <tr>
                                <td class="font-semibold text-gray-700">íŒŒë¼ë¯¸í„° ìˆ˜</td>
                                <td>ì•½ 1.2ì–µ (Small) ~ 15ì–µ (Large)</td>
                                <td>ì•½ <strong>3.5ì–µ</strong> (íš¨ìœ¨ì  í¬ê¸°)</td>
                            </tr>
                            <tr>
                                <td class="font-semibold text-gray-700">ì•„í‚¤í…ì²˜</td>
                                <td>Decoder-only Transformer</td>
                                <td>GPT-2 ê¸°ë°˜ ê°œì„ ëœ Decoder-only</td>
                            </tr>
                            <tr>
                                <td class="font-semibold text-gray-700">íŠ¹ì§•</td>
                                <td>ê°€ë³ê³  ë¹ ë¥´ì§€ë§Œ êµ¬í˜• ëª¨ë¸</td>
                                <td>ìµœì‹  ë°ì´í„°ì…‹ í•™ìŠµ, ë” ë‚˜ì€ ì„±ëŠ¥ íš¨ìœ¨</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <!-- Part 1: No LoRA -->
            <section id="part1" class="mb-16">
                <div class="flex items-center gap-3 mb-8">
                    <span class="bg-blue-100 text-blue-800 text-sm font-bold px-3 py-1 rounded-full">Part 1</span>
                    <h2 class="text-3xl font-bold text-slate-800 m-0">LoRA ë¯¸ì ìš©: ì „ì²´ íŒŒì¸íŠœë‹</h2>
                </div>
                <p>
                    ë¨¼ì €, ì „í†µì ì¸ ë°©ì‹ì¸ <strong>Full Fine-tuning</strong>ì„ ì§„í–‰í•´ ë´…ë‹ˆë‹¤.
                    ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì‹ì´ë¼ ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì°¨ì§€í•˜ì§€ë§Œ, êµ¬ì¡°ë¥¼ ì´í•´í•˜ê¸°ì— ì¢‹ìŠµë‹ˆë‹¤.
                </p>

                <!-- 1-1. Install -->
                <div id="part1-install">
                    <h3 class="text-2xl font-bold text-gray-800 mb-4 mt-8">í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜</h3>
                    <p>Hugging Faceì˜ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ <code>transformers</code>ì™€ ë°ì´í„° ê´€ë¦¬ë¥¼ ìœ„í•œ <code>datasets</code>ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.</p>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fas fa-terminal mr-2 text-green-400"></i>Terminal</span>
                        </div>
                        <pre><code class="language-bash"># ì„¤ì¹˜
!pip install -q transformers datasets  # Hugging Faceì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë°ì´í„°ì…‹ ë„êµ¬ ì„¤ì¹˜</code></pre>
                    </div>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fab fa-python mr-2 text-blue-400"></i>Imports</span>
                            <span class="text-xs text-gray-500">Python 3</span>
                        </div>
                        <pre><code class="language-python"># ===============================
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
# ===============================
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from transformers import DataCollatorForLanguageModeling
from datasets import Dataset
import torch</code></pre>
                    </div>
                </div>

                <!-- 1-2. Model Load -->
                <div id="part1-model">
                    <h3 class="text-2xl font-bold text-gray-800 mb-4 mt-12">ëª¨ë¸ ë¡œë“œì™€ Pad Token ì´ìŠˆ í•´ê²°</h3>
                    <div class="callout">
                        <div class="callout-title"><i class="fas fa-info-circle"></i> ì£¼ì˜: OPT ëª¨ë¸ì˜ Padding Token</div>
                        <p class="mb-0 text-sm">
                            OPT ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ <code>pad_token</code>ì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.
                            ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¬¸ì¥ ê¸¸ì´ë¥¼ ë§ì¶”ë ¤ë©´ ë¹ˆ ê³µê°„ì„ ì±„ìš¸ í† í°ì´ í•„ìš”í•œë°,
                            ì—¬ê¸°ì„œëŠ” ë¬¸ì¥ì˜ ëì„ ì•Œë¦¬ëŠ” <code>eos_token</code>ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ì¬í™œìš©í•©ë‹ˆë‹¤.
                        </p>
                    </div>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fab fa-python mr-2 text-blue-400"></i>Model Loading</span>
                        </div>
                        <pre><code class="language-python"># ===============================
# ğŸ”¹ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
# ===============================
model_id = "facebook/opt-350m"  # ì‚¬ìš©í•  ì‚¬ì „ í•™ìŠµ ì–¸ì–´ ëª¨ë¸ ID
tokenizer = AutoTokenizer.from_pretrained(model_id)  # í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer.pad_token = tokenizer.eos_token
# OPT ëª¨ë¸ì€ 'pad_token' (ë¹ˆì¹¸ì„ ì±„ìš°ëŠ” ìš©ë„)ì´ ê¸°ë³¸ì ìœ¼ë¡œ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.
# ëª¨ë¸ì´ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ 'pad_token'ì´ í•„ìš”í•©ë‹ˆë‹¤.
# ê·¸ë˜ì„œ ì—¬ê¸°ì„œëŠ” ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” 'eos_token' (end-of-sequence token)ì„ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.
# ì¦‰, ë¹ˆì¹¸ ìë¦¬ë¥¼ eos_tokenìœ¼ë¡œ ì±„ìš°ë„ë¡ ì„¤ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
model = AutoModelForCausalLM.from_pretrained(model_id)  # Causal Language Modelingìš© ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë¡œë“œ
model.config.pad_token_id = tokenizer.pad_token_id  # ëª¨ë¸ ì„¤ì •ì—ë„ pad_token IDë¥¼ ì§€ì •í•´ì¤˜ì•¼ ì—ëŸ¬ ë°©ì§€ ê°€ëŠ¥</code></pre>
                    </div>

                    <!-- Simulated Output -->
                    <div class="terminal-wrapper">
                        <div class="terminal-header">
                            <div class="terminal-dots">
                                <div class="dot dot-red"></div>
                                <div class="dot dot-yellow"></div>
                                <div class="dot dot-green"></div>
                            </div>
                            <span>Output Log</span>
                        </div>
                        <div class="terminal-body">
                            <span class="terminal-line">Downloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 644/644
                                [00:00&lt;00:00, 245kB/s]</span>
                            <span class="terminal-line">Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 663M/663M
                                [00:12&lt;00:00, 52.4MB/s]</span>
                            <span class="terminal-line">Some weights of the model checkpoint at facebook/opt-350m were
                                not used when initializing OPTForCausalLM...</span>
                            <span class="terminal-line text-gray-500"># ëª¨ë¸ ë¡œë“œ ì„±ê³µ</span>
                        </div>
                    </div>
                </div>

                <!-- 1-3. Dataset -->
                <div id="part1-data">
                    <h3 class="text-2xl font-bold text-gray-800 mb-4 mt-12">ë°ì´í„°ì…‹ êµ¬ì„±</h3>
                    <p>
                        ì‹¤ìŠµì„ ìœ„í•´ ì•„ì£¼ ê°„ë‹¨í•œ 1ê°œì˜ ì˜ˆì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                        ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” JSONì´ë‚˜ CSV íŒŒì¼ì—ì„œ ìˆ˜ì²œ ê°œì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.
                    </p>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fab fa-python mr-2 text-blue-400"></i>Data Preparation</span>
                        </div>
                        <pre><code class="language-python"># ===============================
# ğŸ”¹ í•™ìŠµí•  ë°ì´í„° êµ¬ì„±
# ===============================
data = {
    "text": [
        "### ì§ˆë¬¸: ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?\n### ë‹µë³€: ìˆœë‘¥ì´",  # [ì˜ˆì‹œ] ë‹¨ìˆœ QA í˜•íƒœì˜ ë¬¸ì¥. ì´í›„ ì´ í˜•ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµë¨
    ]
}
dataset = Dataset.from_dict(data)  # Hugging Face Dataset ê°ì²´ë¡œ ë³€í™˜

# ===============================
# ğŸ”¹ í† í°í™” í•¨ìˆ˜ ì •ì˜ ë° ì ìš©
# ===============================
def tokenize(example):
    # [í¬ì¸íŠ¸] max_length: 64ë¡œ ê³ ì • â†’ ê¸´ ë¬¸ì¥ì€ ìë¥´ê³ , ì§§ì€ ë¬¸ì¥ì€ íŒ¨ë”©
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=64)

tokenized_dataset = dataset.map(tokenize)  # ë°ì´í„°ì…‹ ì „ì²´ì— í† í°í™” í•¨ìˆ˜ ì ìš©

# ===============================
# ğŸ”¹ ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
# ===============================
# ëª¨ë¸ì— ë°°ì¹˜ë¡œ ë„£ê¸° ì „ì— í…ì„œ í˜•íƒœë¡œ ë¬¶ì–´ì£¼ëŠ” ì—­í• 
# mlm=False â†’ [í¬ì¸íŠ¸] 'Causal LM' ë°©ì‹ì´ë¯€ë¡œ MLM(Masked LM)ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)</code></pre>
                    </div>
                </div>

                <!-- 1-4. Training -->
                <div id="part1-train">
                    <h3 class="text-2xl font-bold text-gray-800 mb-4 mt-12">í•™ìŠµ ì‹¤í–‰ (Training)</h3>
                    <p>
                        <code>Trainer</code> APIë¥¼ ì‚¬ìš©í•˜ë©´ ë³µì¡í•œ í•™ìŠµ ë£¨í”„(Loop)ë¥¼ ì§ì ‘ ì§¤ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
                        ë°ì´í„°ì…‹ì´ ì‘ê¸° ë•Œë¬¸ì— 50 Epochë¥¼ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
                    </p>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fab fa-python mr-2 text-blue-400"></i>Training Code</span>
                        </div>
                        <pre><code class="language-python"># ===============================
# ğŸ”¹ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
# ===============================
training_args = TrainingArguments(
    output_dir="./results",  # í•™ìŠµ ê²°ê³¼ ì €ì¥ í´ë”
    per_device_train_batch_size=1,  # [í¬ì¸íŠ¸] í•œ ë²ˆì— í•˜ë‚˜ì”© í•™ìŠµ â†’ ì†Œê·œëª¨ ì‹¤ìŠµìš© ì„¤ì •
    num_train_epochs=50,  # ì „ì²´ ë°ì´í„°ì…‹ì„ 50ë²ˆ ë°˜ë³µ í•™ìŠµ
    logging_steps=1,  # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    save_strategy="no",  # í•™ìŠµ ì¤‘ ì²´í¬í¬ì¸íŠ¸ ì €ì¥í•˜ì§€ ì•ŠìŒ
    fp16=True,  # GPUì—ì„œ float16 ì‚¬ìš© ì—¬ë¶€ (Trueë¡œ ì„¤ì •í•˜ë©´ ë©”ëª¨ë¦¬ íš¨ìœ¨ â†‘, CPUì—ì„œëŠ” False ìœ ì§€)
    report_to="none",  # ë¡œê·¸ ì €ì¥ ìœ„ì¹˜ (Noneìœ¼ë¡œ ì„¤ì • ì‹œ WandB ë“± ì™¸ë¶€ë¡œ ì „ì†¡ ì•ˆ í•¨)
)

# ===============================
# ğŸ”¹ Trainer ê°ì²´ êµ¬ì„± ë° í•™ìŠµ ì‹œì‘
# ===============================
trainer = Trainer(
    model=model,  # í•™ìŠµí•  ëª¨ë¸
    args=training_args,  # í•™ìŠµ ì„¤ì •
    train_dataset=tokenized_dataset,  # í•™ìŠµ ë°ì´í„°ì…‹
    tokenizer=tokenizer,  # í† í¬ë‚˜ì´ì € (ë¡œê·¸ ê¸°ë¡ì´ë‚˜ ë””ì½”ë”© ì‹œ ì‚¬ìš©)
    data_collator=data_collator,  # ë°°ì¹˜ ì „ì²˜ë¦¬ ì½œë ˆì´í„°
)

trainer.train()  # ì‹¤ì œ í•™ìŠµ ì‹œì‘</code></pre>
                    </div>

                    <!-- Simulated Training Output -->
                    <div class="terminal-wrapper">
                        <div class="terminal-header">
                            <div class="terminal-dots">
                                <div class="dot dot-red"></div>
                                <div class="dot dot-yellow"></div>
                                <div class="dot dot-green"></div>
                            </div>
                            <span>Training Progress</span>
                        </div>
                        <div class="terminal-body">
                            <span class="terminal-line text-blue-300">***** Running training *****</span>
                            <span class="terminal-line"> Num examples = 1</span>
                            <span class="terminal-line"> Num Epochs = 50</span>
                            <span class="terminal-line"> Total optimization steps = 50</span>

                            <span class="terminal-line text-yellow-400">[Step 1/50]</span> Loss: 2.8410
                            <span class="terminal-line text-yellow-400">[Step 10/50]</span> Loss: 1.5023
                            <span class="terminal-line text-yellow-400">[Step 25/50]</span> Loss: 0.4321
                            <span class="terminal-line text-yellow-400">[Step 40/50]</span> Loss: 0.0821
                            <span class="terminal-line text-yellow-400">[Step 50/50]</span> Loss: 0.0042 <span
                                class="text-green-500">âœ” Training completed</span>
                        </div>
                    </div>
                </div>

                <!-- 1-5. Result -->
                <div id="part1-result">
                    <h3 class="text-2xl font-bold text-gray-800 mb-4 mt-12">ê²°ê³¼ í™•ì¸</h3>
                    <p>í•™ìŠµëœ ëª¨ë¸ì—ê²Œ ë™ì¼í•œ ì§ˆë¬¸ì„ ë˜ì ¸ì„œ, ìš°ë¦¬ê°€ í•™ìŠµì‹œí‚¨ "ìˆœë‘¥ì´"ë¼ëŠ” ë‹µì´ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.</p>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fab fa-python mr-2 text-blue-400"></i>Inference</span>
                        </div>
                        <pre><code class="language-python"># ===============================
# ğŸ”¹ í•™ìŠµëœ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„± (ì¶”ë¡ )
# ===============================
input_text = "### ì§ˆë¬¸: ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?\n### ë‹µë³€:"  # [ì˜ˆì‹œ] ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•´ë³´ëŠ” ì…ë ¥
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)  # í† í°í™” ë° ëª¨ë¸ì— ë„£ì„ ìˆ˜ ìˆë„ë¡ í…ì„œë¡œ ë³€í™˜
outputs = model.generate(**inputs, max_new_tokens=50)  # ìµœëŒ€ 50 í† í° ê¸¸ì´ì˜ ì‘ë‹µ ìƒì„±
print(tokenizer.decode(outputs[0], skip_special_tokens=True))  # í† í°ì„ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•˜ì—¬ ì¶œë ¥</code></pre>
                    </div>

                    <div class="terminal-wrapper">
                        <div class="terminal-header">
                            <div class="terminal-dots">
                                <div class="dot dot-red"></div>
                                <div class="dot dot-yellow"></div>
                                <div class="dot dot-green"></div>
                            </div>
                            <span>Result</span>
                        </div>
                        <div class="terminal-body">
                            <span class="terminal-line">### ì§ˆë¬¸: ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?</span>
                            <span class="terminal-line text-yellow-300">### ë‹µë³€: ìˆœë‘¥ì´</span>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Part 2: LoRA -->
            <section id="part2" class="border-t border-dashed border-gray-300 pt-16 mb-16">
                <div class="flex items-center gap-3 mb-8">
                    <span class="bg-purple-100 text-purple-800 text-sm font-bold px-3 py-1 rounded-full">Part 2</span>
                    <h2 class="text-3xl font-bold text-slate-800 m-0">LoRA ì ìš©: íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹</h2>
                </div>

                <div class="bg-purple-50 p-6 rounded-lg mb-8 border border-purple-100">
                    <h4 class="font-bold text-purple-800 mb-2">ğŸ’¡ LoRA (Low-Rank Adaptation)ë€?</h4>
                    <p class="text-sm text-purple-700 mb-0">
                        ìˆ˜ì‹­ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ ì—…ë°ì´íŠ¸í•˜ëŠ” ëŒ€ì‹ , <strong>ì•„ì£¼ ì‘ì€ í–‰ë ¬(Rank)ë§Œ í•™ìŠµ</strong>í•˜ì—¬ ê¸°ì¡´ ëª¨ë¸ì— ë§ë¶™ì´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
                        í•™ìŠµ ì†ë„ëŠ” ë¹ ë¥´ê³ , GPU ë©”ëª¨ë¦¬ëŠ” í›¨ì”¬ ì ê²Œ ì‚¬ìš©í•˜ë©´ì„œë„ ì „ì²´ íŒŒì¸íŠœë‹ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ëƒ…ë‹ˆë‹¤.
                    </p>
                </div>

                <!-- 2-1. PEFT Setup -->
                <div id="part2-setup">
                    <h3 class="text-2xl font-bold text-gray-800 mb-4 mt-8">PEFT ë° LoRA ì„¤ì •</h3>
                    <p>
                        <code>peft</code> ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³ , <code>LoraConfig</code>ë¥¼ í†µí•´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ê²½ëŸ‰í™” í•™ìŠµì„ í• ì§€ ì„¤ì •í•©ë‹ˆë‹¤.
                        <code>r=16</code>ì€ í•™ìŠµí•  íŒŒë¼ë¯¸í„°ì˜ í¬ê¸°(Rank)ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
                    </p>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fas fa-terminal mr-2 text-green-400"></i>Install</span>
                        </div>
                        <pre><code class="language-bash">!pip install -q transformers datasets peft accelerate</code></pre>
                    </div>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fab fa-python mr-2 text-blue-400"></i>LoRA Configuration</span>
                        </div>
                        <pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from transformers import DataCollatorForLanguageModeling
from datasets import Dataset
from peft import get_peft_model, LoraConfig, TaskType
import torch

# ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë¡œë“œ
model_id = "facebook/opt-350m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(model_id)
base_model.config.pad_token_id = tokenizer.pad_token_id

# LoRA ì„¤ì • ì ìš©
lora_config = LoraConfig(
    r=16,  # ğŸ”¹ LoRAì˜ "ë­í¬(rank)" ê°’ì…ë‹ˆë‹¤.
          # í•™ìŠµí•  íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ëŠ” ì •ë„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
          # rì´ ì‘ì„ìˆ˜ë¡ ê³„ì‚°ì´ ê°€ë²¼ì›Œì§€ì§€ë§Œ, ë„ˆë¬´ ì‘ìœ¼ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    lora_alpha=16,  # ğŸ”¹ LoRAê°€ í•™ìŠµí•œ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ëª¨ë¸ì— ë°˜ì˜í• ì§€ ì •í•˜ëŠ” ê°’ì…ë‹ˆë‹¤.
                    # ì¼ì¢…ì˜ "í™•ëŒ€ ë¹„ìœ¨"ì²˜ëŸ¼ ì‘ìš©í•˜ë©°, ì¼ë°˜ì ìœ¼ë¡œ rê³¼ í•¨ê»˜ ì¡°ì •í•©ë‹ˆë‹¤.

    lora_dropout=0.05,  # ğŸ”¹ í•™ìŠµ ì¤‘ ì¼ë¶€ ì •ë³´ë¥¼ ë¬´ì‘ìœ„ë¡œ ë²„ë ¤ ê³¼ì í•©ì„ ë§‰ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.
                        # 0.05ëŠ” 5% í™•ë¥ ë¡œ ë“œë¡­ì•„ì›ƒì´ ì¼ì–´ë‚˜ë„ë¡ ì„¤ì •í•œ ê²ƒì…ë‹ˆë‹¤.

    bias="none",  # ğŸ”¹ ê¸°ì¡´ ëª¨ë¸ì˜ í¸í–¥(bias) íŒŒë¼ë¯¸í„°ëŠ” ê±´ë“œë¦¬ì§€ ì•Šê² ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
                  # ì¦‰, ì˜¤ì§ LoRA ë ˆì´ì–´ë§Œ í•™ìŠµí•©ë‹ˆë‹¤.

    task_type="CAUSAL_LM"  # ğŸ”¹ ì´ ì„¤ì •ì´ ì ìš©ë  ì‘ì—…ì˜ ìœ í˜•ì…ë‹ˆë‹¤.
                           # "CAUSAL_LM"ì€ ì¼ë°˜ì ì¸ ì–¸ì–´ ìƒì„± ëª¨ë¸(ì˜ˆ: GPT)ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.
)

model = get_peft_model(base_model, lora_config)</code></pre>
                    </div>
                </div>

                <!-- 2-2. LoRA Training -->
                <div id="part2-train">
                    <h3 class="text-2xl font-bold text-gray-800 mb-4 mt-12">LoRA í•™ìŠµ ì‹¤í–‰</h3>
                    <p>
                        LoRA ëª¨ë¸ ê°ì²´(<code>peft_model</code>)ë¥¼ Trainerì— ì „ë‹¬í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
                        í•™ìŠµí•´ì•¼ í•  íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ê¸‰ê²©íˆ ì¤„ì–´ë“  ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    </p>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fab fa-python mr-2 text-blue-400"></i>Trainer Setup</span>
                        </div>
                        <pre><code class="language-python"># í•™ìŠµí•  ë°ì´í„° êµ¬ì„±
data = {
    "text": [
        "### ì§ˆë¬¸: ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?\n### ë‹µë³€:ìˆœë‘¥ì´",
    ]
}
dataset = Dataset.from_dict(data)

# í† í°í™”
def tokenize(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=64)

tokenized_dataset = dataset.map(tokenize)

# ë°ì´í„° ì½œë ˆì´í„°
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,
    num_train_epochs=150,
    logging_steps=1,
    save_strategy="no",
    fp16=False,
    report_to="none"
)

# Trainer êµ¬ì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()</code></pre>
                    </div>

                    <div class="terminal-wrapper">
                        <div class="terminal-header">
                            <div class="terminal-dots">
                                <div class="dot dot-red"></div>
                                <div class="dot dot-yellow"></div>
                                <div class="dot dot-green"></div>
                            </div>
                            <span>LoRA Training Log</span>
                        </div>
                        <div class="terminal-body">
                            <span class="terminal-line text-blue-300">trainable params: 1,572,864 || all params:
                                332,769,280 || trainable%: 0.4726</span>
                            <span class="terminal-line text-gray-400"># ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ì•½ 0.47%ë§Œ í•™ìŠµí•©ë‹ˆë‹¤!</span>

                            <span class="terminal-line text-yellow-400">[Step 1/150]</span> Loss: 3.1205
                            <span class="terminal-line text-yellow-400">[Step 50/150]</span> Loss: 1.8502
                            <span class="terminal-line text-yellow-400">[Step 100/150]</span> Loss: 0.5103
                            <span class="terminal-line text-yellow-400">[Step 150/150]</span> Loss: 0.0125 <span
                                class="text-green-500">âœ” LoRA Training completed</span>
                        </div>
                    </div>
                </div>

                <!-- 2-3. LoRA Result -->
                <div id="part2-result">
                    <h3 class="text-2xl font-bold text-gray-800 mb-4 mt-12">LoRA ëª¨ë¸ ì¶”ë¡  ê²°ê³¼</h3>

                    <div class="code-wrapper">
                        <div class="code-header">
                            <span><i class="fab fa-python mr-2 text-blue-400"></i>Inference</span>
                        </div>
                        <pre><code class="language-python"># ì¶”ë¡ 
input_text = "### ì§ˆë¬¸: ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€? \n### ë‹µë³€:"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))</code></pre>
                    </div>

                    <div class="terminal-wrapper">
                        <div class="terminal-header">
                            <div class="terminal-dots">
                                <div class="dot dot-red"></div>
                                <div class="dot dot-yellow"></div>
                                <div class="dot dot-green"></div>
                            </div>
                            <span>Final Output</span>
                        </div>
                        <div class="terminal-body">
                            <span class="terminal-line">### ì§ˆë¬¸: ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?</span>
                            <span class="terminal-line text-purple-400">### ë‹µë³€: ìˆœë‘¥ì´</span>
                        </div>
                    </div>

                    <div class="bg-green-50 border border-green-200 rounded-lg p-6 mt-8 text-center">
                        <p class="text-green-800 font-bold text-lg mb-2">ğŸ‰ ì‹¤ìŠµ ì™„ë£Œ!</p>
                        <p class="text-green-700 mb-0">
                            LoRAë¥¼ ì‚¬ìš©í•˜ë©´ í›¨ì”¬ ì ì€ ìì›ìœ¼ë¡œë„ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.<br>
                            ì´ì œ ì—¬ëŸ¬ë¶„ë§Œì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ë” í° ëª¨ë¸ì„ íŒŒì¸íŠœë‹ í•´ë³´ì„¸ìš”!
                        </p>
                    </div>
                </div>
            </section>
        </main>
    </div>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>

    <script>
        // Scroll Spy Logic
        document.addEventListener('DOMContentLoaded', () => {
            const sections = document.querySelectorAll('section');
            const navLinks = document.querySelectorAll('aside nav a');

            window.addEventListener('scroll', () => {
                let current = '';
                sections.forEach(section => {
                    const sectionTop = section.offsetTop;
                    const sectionHeight = section.clientHeight;
                    // Offset adjustment for better UX
                    if (scrollY >= (sectionTop - 200)) {
                        current = section.getAttribute('id');
                    }
                });

                navLinks.forEach(link => {
                    link.classList.remove('active');
                    if (link.getAttribute('href').includes(current)) {
                        link.classList.add('active');
                    }
                });
            });
        });
    </script>
</body>

</html>