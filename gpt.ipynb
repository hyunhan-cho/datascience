{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GPT-2 Fine-Tuning with LoRA Lab\n",
                "\n",
                "ì´ ë…¸íŠ¸ë¶ì€ `gpt.html` ê°€ì´ë“œì™€ ì—°ë™ë˜ëŠ” ì‹¤ìŠµ ì½”ë“œì…ë‹ˆë‹¤.\n",
                "GPT-2 ëª¨ë¸ì— **LoRA (Low-Rank Adaptation)** ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ë§¤ìš° ì ì€ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
                "\n",
                "**ëª©í‘œ**: ëª¨ë¸ì—ê²Œ \"ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?\"ì´ë¼ê³  ë¬¼ì—ˆì„ ë•Œ \"ìˆœë‘¥ì´\"ë¼ê³  ë‹µí•˜ë„ë¡ í•™ìŠµì‹œí‚¤ê¸°"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
                        "\u001b[?25h"
                    ]
                }
            ],
            "source": [
                "# STEP 1: í™˜ê²½ ì„¤ì •\n",
                "%pip install -q transformers datasets peft accelerate bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 2: ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
                "import torch  # PyTorch: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬\n",
                "from datasets import Dataset  # ë°ì´í„°ì…‹ ì²˜ë¦¬\n",
                "from transformers import (\n",
                "    AutoTokenizer, \n",
                "    AutoModelForCausalLM, \n",
                "    TrainingArguments, \n",
                "    Trainer, \n",
                "    DataCollatorForLanguageModeling\n",
                ")\n",
                "from peft import get_peft_model, LoraConfig, TaskType  # LoRA í•µì‹¬ ëª¨ë“ˆ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 3: ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
                "model_id = \"gpt2\"\n",
                "\n",
                "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "\n",
                "# ğŸ’¡ ì¤‘ìš”: GPT-2ëŠ” pad_tokenì´ ì—†ì–´ì„œ eos_tokenìœ¼ë¡œ ëŒ€ì²´ ì„¤ì •\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# ëª¨ë¸ ë¡œë“œ (CausalLM: ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§ìš©)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_id)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 4: LoRA ì„¤ì •\n",
                "lora_config = LoraConfig(\n",
                "    r=16,             # LoRA Rank\n",
                "    lora_alpha=8,     # ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜\n",
                "    target_modules=[\"c_attn\", \"c_proj\", \"q_attn\"], # ì ìš©í•  ë ˆì´ì–´\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=TaskType.CAUSAL_LM,\n",
                ")\n",
                "\n",
                "# ëª¨ë¸ì— LoRA ì ìš©\n",
                "model = get_peft_model(model, lora_config)\n",
                "\n",
                "# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 5: í•™ìŠµ ë°ì´í„°ì…‹ ì •ì˜\n",
                "data = {\n",
                "    \"text\": [\n",
                "        \"### ì§ˆë¬¸: ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?\\n### ë‹µë³€: ìˆœë‘¥ì´\",\n",
                "        \"### ì§ˆë¬¸: ë°”ë‹¤ëŠ” ì™œ íŒŒë€ê°€ìš”?\\n### ë‹µë³€: í–‡ë¹›ì˜ ì‚°ë€\",\n",
                "    ]\n",
                "}\n",
                "dataset = Dataset.from_dict(data)\n",
                "\n",
                "# STEP 6: í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜\n",
                "def tokenize_function(example):\n",
                "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
                "\n",
                "tokenized_dataset = dataset.map(tokenize_function)\n",
                "\n",
                "# STEP 7: ë°ì´í„° ì½œë ˆì´í„°\n",
                "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 8: íŠ¸ë ˆì´ë‹ ì„¤ì •\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    per_device_train_batch_size=10,\n",
                "    num_train_epochs=50,  # 50ë²ˆ ë°˜ë³µ í•™ìŠµ\n",
                "    logging_steps=1,\n",
                "    save_strategy=\"no\",\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "# ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜\n",
                "def compute_metrics(eval_pred):\n",
                "    logits, labels = eval_pred\n",
                "    predictions = np.argmax(logits, axis=-1)\n",
                "    mask = labels != -100  # íŒ¨ë”© ì œì™¸\n",
                "    correct = (predictions == labels) & mask\n",
                "    accuracy = correct.sum() / mask.sum()\n",
                "    return {\"accuracy\": accuracy}\n",
                "\n",
                "# STEP 9: íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë° í•™ìŠµ ì‹œì‘\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    compute_metrics=compute_metrics\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STEP 10: ì¶”ë¡  ì˜ˆì‹œ\n",
                "input_text = \"### ì§ˆë¬¸: ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?\\n### ë‹µë³€:\"\n",
                "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
                "\n",
                "# GPUë¡œ ì´ë™\n",
                "device = model.device\n",
                "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                "\n",
                "# ìƒì„±\n",
                "outputs = model.generate(**inputs, max_new_tokens=50)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
