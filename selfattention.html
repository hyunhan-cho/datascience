<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer: Attention Is All You Need</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Pretendard:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0KOVEMckDpbDM7QIjQRubAOKAIaxcFiaLotxbbTMW" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzYCEHPNKAev8tduzqlIfrFv4EbvrLGIkrg" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError: false
        });
    });
    </script>
    <style>
        body { font-family: 'Pretendard', sans-serif; background-color: #f8fafc; }
        .presentation-container {
            width: 100%;
            max-width: 1280px;
            margin: auto;
            background-color: #ffffff;
            border-radius: 12px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.07);
            overflow: hidden;
        }
        .slide {
            padding: 3rem 4rem;
            display: none;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            min-height: 700px;
        }
        .slide.active { display: flex; }
        h1 { font-size: 3rem; font-weight: 800; color: #1e293b; text-align: center; }
        h2 { font-size: 2.25rem; font-weight: 700; color: #334155; margin-bottom: 2rem; padding-bottom: 0.75rem; border-bottom: 2px solid #e2e8f0; width: 100%; text-align: center; }
        p { font-size: 1.125rem; line-height: 1.8; color: #475569; text-align: left; max-width: 800px; }
        .formula-box { margin: 1.5rem 0; padding: 1rem 1.5rem; background-color: #f1f5f9; border: 1px solid #e2e8f0; border-radius: 0.75rem; font-size: 1.25rem; text-align: center; overflow-x: auto; }
        .navigation { padding: 1.5rem 3rem; background-color: #f1f5f9; border-top: 1px solid #e2e8f0; }
        .progress-bar-container { background-color: #e2e8f0; border-radius: 9999px; height: 8px; margin-bottom: 1rem; overflow: hidden; }
        .progress-bar { background: linear-gradient(to right, #4f46e5, #a78bfa); height: 100%; border-radius: 9999px; transition: width 0.4s ease; }
        .nav-button { background-color: #4f46e5; color: white; border-radius: 9999px; font-weight: 600; cursor: pointer; transition: all 0.2s ease-in-out; padding: 0.75rem 2rem; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); }
        .nav-button:hover { background-color: #4338ca; transform: translateY(-2px); box-shadow: 0 7px 10px rgba(0, 0, 0, 0.1); }
        .nav-button:disabled { background-color: #94a3b8; cursor: not-allowed; transform: none; box-shadow: none; }
        .grid-item { background-color: #f8fafc; padding: 1.5rem; border-radius: 0.75rem; border: 1px solid #e2e8f0; }
        .grid-item h3 { color: #4338ca; font-size: 1.25rem; font-weight: 600; margin-bottom: 0.5rem; }
        .grid-item p { font-size: 1rem; color: #475569; }
        .diagram { max-width: 100%; height: auto; border: 1px solid #e2e8f0; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.08); margin: 0 auto; display: block; }
        .diagram-container { display: flex; justify-content: center; align-items: center; }
    </style>
</head>
<body class="flex items-center justify-center min-h-screen p-4">
    <div class="presentation-container">
        <div id="slides-wrapper">

            <!-- Slide 1: Title --><div class="slide active" id="slide-0">
                <div class="slide-content text-center">
                    <svg class="w-24 h-24 mx-auto mb-4 text-indigo-600" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 10V3L4 14h7v7l9-11h-7z"></path></svg>
                    <h1 class="font-extrabold text-5xl text-gray-800">The Transformer</h1>
                    <p class="text-2xl mt-4 text-gray-600">Attention Is All You Need</p>
                    <p class="text-sm mt-8 text-gray-500">2017년 Google 연구팀이 발표한 딥러닝 모델의 혁명</p>
                </div>
            </div>

            <!-- Slide 2: The Big Picture --><div class="slide" id="slide-1">
                <div class="slide-content w-full">
                    <h2>Transformer 기본 구조: 인코더-디코더</h2>
                    <p class="text-center text-lg max-w-3xl mx-auto">트랜스포머는 RNN의 순차적 처리 방식에서 벗어나, 전체 시퀀스를 한 번에 처리하는 병렬성에 중점을 둔 모델입니다. 이는 주로 <b>인코더(Encoder)</b>와 <b>디코더(Decoder)</b> 두 부분으로 구성됩니다.</p>
                    <div class="diagram-container my-8">
                        <img src="https://tutorials.pytorch.kr/_images/transformer_architecture.jpg" alt="Transformer Architecture" class="diagram max-w-2xl" >
                    </div>
                    <p class="text-center max-w-3xl mx-auto text-gray-600"><b>인코더</b>는 입력 문장의 의미를 문맥 정보가 풍부한 벡터 표현으로 변환합니다. <b>디코더</b>는 이 벡터와 이전에 생성된 단어들을 기반으로 다음 단어를 예측하여 출력 시퀀스를 만듭니다.</p>
                </div>
            </div>

            <!-- Slide 3: Self-Attention --><div class="slide" id="slide-2">
                <div class="slide-content w-full">
                    <h2>핵심 개념: Self-Attention</h2>
                    <div class="grid md:grid-cols-2 gap-8 items-center">
                        <div class="text-left space-y-4">
                             <p>Self-Attention은 문장 내에서 단어들이 서로에게 얼마나 '주의(Attention)'를 기울여야 하는지 계산하는 메커니즘입니다. 이를 통해 각 단어의 풍부한 문맥적 표현을 생성합니다.</p>
                             <div class="grid-item">
                                 <h3>Query, Key, Value</h3>
                                 <p>모든 입력 벡터는 세 가지 다른 벡터로 변환됩니다. <br>• <b>Query (Q):</b> 현재 단어의 '질문' <br>• <b>Key (K):</b> 각 단어의 '식별표' <br>• <b>Value (V):</b> 각 단어의 '실제 의미'</p>
                            </div>
                        </div>
                        <div class="flex justify-center items-center">
                            <svg class="w-full max-w-sm" viewBox="0 0 400 200" xmlns="http://www.w3.org/2000/svg">
                                <defs>
                                    <marker id="arrowhead-qkv" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto" fill="#4a5568">
                                        <polygon points="0 0, 10 3.5, 0 7" />
                                    </marker>
                                    <linearGradient id="gradQ" x1="0%" y1="0%" x2="100%" y2="0%"><stop offset="0%" stop-color="#3b82f6"/><stop offset="100%" stop-color="#2563eb"/></linearGradient>
                                    <linearGradient id="gradK" x1="0%" y1="0%" x2="100%" y2="0%"><stop offset="0%" stop-color="#22c55e"/><stop offset="100%" stop-color="#16a34a"/></linearGradient>
                                    <linearGradient id="gradV" x1="0%" y1="0%" x2="100%" y2="0%"><stop offset="0%" stop-color="#a855f7"/><stop offset="100%" stop-color="#9333ea"/></linearGradient>
                                </defs>

                                <rect x="150" y="20" width="100" height="40" rx="5" fill="#e2e8f0" stroke="#cbd5e1" stroke-width="1.5"/>
                                <text x="200" y="45" text-anchor="middle" font-size="14" fill="#334155">Input Vector</text>

                                <path d="M200 60 V 80" stroke="#4a5568" stroke-width="2" marker-end="url(#arrowhead-qkv)"/>

                                <g transform="translate(0, 80)">
                                    <!-- Query --><rect x="50" y="0" width="80" height="30" rx="4" fill="url(#gradQ)"/>
                                    <text x="90" y="19" text-anchor="middle" font-size="12" fill="white" font-weight="bold">Q</text>
                                    <path d="M200 -20 C 180 -20, 150 0, 130 0" stroke="#4a5568" stroke-width="1.5" fill="none" marker-end="url(#arrowhead-qkv)"/>

                                    <!-- Key --><rect x="160" y="0" width="80" height="30" rx="4" fill="url(#gradK)"/>
                                    <text x="200" y="19" text-anchor="middle" font-size="12" fill="white" font-weight="bold">K</text>
                                    <path d="M200 -20 C 200 -20, 200 0, 200 0" stroke="#4a5568" stroke-width="1.5" fill="none" marker-end="url(#arrowhead-qkv)"/>

                                    <!-- Value --><rect x="270" y="0" width="80" height="30" rx="4" fill="url(#gradV)"/>
                                    <text x="310" y="19" text-anchor="middle" font-size="12" fill="white" font-weight="bold">V</text>
                                    <path d="M200 -20 C 220 -20, 250 0, 270 0" stroke="#4a5568" stroke-width="1.5" fill="none" marker-end="url(#arrowhead-qkv)"/>
                                </g>

                                <text x="200" y="160" text-anchor="middle" font-size="12" fill="#475569">단어의 Query, Key, Value 벡터 생성</text>
                            </svg>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Slide 4: Scaled Dot-Product Attention --><div class="slide" id="slide-3">
                <div class="slide-content w-full">
                    <h2>어텐션 계산 과정</h2>
                     <p class="text-lg text-center max-w-3xl mx-auto">어텐션은 (1) Query와 Key의 유사도를 계산하고, (2) Softmax로 정규화하여 가중치를 얻은 후, (3) 이 가중치를 Value에 곱해 최종 결과를 얻는 3단계로 이루어집니다.</p>
                    <div class="diagram-container my-8">
                        <img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" alt="Scaled Dot-Product Attention 계산 과정" class="diagram max-w-full">
                    </div>
                    <div class="formula-box w-full max-w-xl mx-auto">
                        $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
                    </div>
                </div>
            </div>

            <!-- Slide 5: Multi-Head Attention --><div class="slide" id="slide-4">
                <div class="slide-content w-full">
                    <h2>Multi-Head Attention</h2>
                    <div class="grid md:grid-cols-2 gap-8 items-center">
                        <div class="text-left space-y-4">
                            <p>하나의 어텐션만 사용하는 대신, 여러 개의 어텐션(Head)을 병렬로 수행합니다. 각 헤드는 서로 다른 관점에서 단어 간의 관계를 학습하여, 모델이 더 풍부하고 다양한 정보를 포착할 수 있게 합니다.</p>
                            <p class="text-gray-700">각 헤드의 출력 벡터들을 하나로 합친(Concatenate) 후, 최종 선형 변환을 통해 최종 결과 벡터를 생성합니다.</p>
                            <div class="formula-box">
                                 $$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O $$
                                 $$ \text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$
                             </div>
                        </div>
                        <div class="flex justify-center items-center">
                            <img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="Multi-Head Attention 구조" class="diagram max-w-sm">
                        </div>
                    </div>
                </div>
            </div>

            <!-- Slide 6: Positional Encoding --><div class="slide" id="slide-5">
                <div class="slide-content w-full">
                    <h2>위치 정보 추가: Positional Encoding</h2>
                    <div class="grid md:grid-cols-2 gap-8 items-center">
                        <div class="text-left space-y-4">
                            <p>트랜스포머는 순서 개념이 없기 때문에, 단어의 위치 정보를 인위적으로 주입해야 합니다. 이를 위해 <b>포지셔널 인코딩(Positional Encoding)</b> 벡터를 단어 임베딩에 더해줍니다.</p>
                            <p class="text-gray-700 mt-4">사인(sin)과 코사인(cos) 함수를 사용하여 각 위치마다 고유한 패턴을 가지는 벡터를 생성합니다. 이를 통해 모델은 단어의 상대적인 위치 관계를 학습할 수 있습니다.</p>
                            <div class="formula-box text-sm">
                                $$ PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}}) $$
                                $$ PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}}) $$
                            </div>
                        </div>
                        <div class="flex justify-center items-center">
                            <svg class="w-full max-w-sm diagram" viewBox="0 0 400 250" xmlns="http://www.w3.org/2000/svg">
                                <defs>
                                    <linearGradient id="gradPos" x1="0%" y1="0%" x2="100%" y2="0%"><stop offset="0%" stop-color="#a78bfa"/><stop offset="100%" stop-color="#8b5cf6"/></linearGradient>
                                </defs>
                                <rect x="50" y="50" width="100" height="40" rx="5" fill="#e0e7ff" stroke="#a5b4fc" stroke-width="1.5"/>
                                <text x="100" y="75" text-anchor="middle" font-size="14" fill="#334155">Token Emb.</text>

                                <rect x="180" y="50" width="100" height="40" rx="5" fill="url(#gradPos)" stroke="#a78bfa" stroke-width="1.5"/>
                                <text x="230" y="75" text-anchor="middle" font-size="14" fill="white" font-weight="bold">Positional Emb.</text>

                                <text x="150" y="100" font-size="20" fill="#4a5568" font-weight="bold" text-anchor="middle">+</text>
                                <text x="280" y="100" font-size="20" fill="#4a5568" font-weight="bold" text-anchor="middle">=</text>

                                <rect x="230" y="120" width="100" height="40" rx="5" fill="#d1fae5" stroke="#6ee7b7" stroke-width="1.5"/>
                                <text x="280" y="145" text-anchor="middle" font-size="14" fill="#334155">Output Emb.</text>

                                <!-- Arrows --><path d="M100 90 V 110 H 140" stroke="#4a5568" stroke-width="1.5" fill="none" marker-end="url(#arrowhead)"/>
                                <path d="M230 90 V 110 H 160" stroke="#4a5568" stroke-width="1.5" fill="none" marker-end="url(#arrowhead)"/>
                                <path d="M280 110 V 120" stroke="#4a5568" stroke-width="1.5" fill="none" marker-end="url(#arrowhead)"/>

                                <g transform="translate(0, 100)">
                                    <text x="200" y="10" text-anchor="middle" font-size="12" fill="#475569">단어 임베딩 + 위치 임베딩</text>
                                </g>
                            </svg>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Slide 7: Encoder & Decoder Stacks --><div class="slide" id="slide-6">
                <div class="slide-content w-full">
                    <h2><b>인코더와 디코더의 상세 구조</b></h2>
                    <p class="text-lg max-w-3xl mx-auto">트랜스포머는 여러 개의 동일한 인코더와 디코더 블록을 쌓아 만듭니다. 각 블록 내부에는 어텐션 메커니즘과 피드 포워드 네트워크가 포함되어 있습니다.</p>
                    <div class="diagram-container my-8">
                        <img src="https://tutorials.pytorch.kr/_images/transformer_architecture.jpg" alt="Transformer Architecture" class="diagram max-w-2xl" >
                    </div>
                    <div class="flex flex-col md:flex-row gap-6 text-left mt-4">
                        <div class="flex-1 grid-item">
                            <h3 class="text-indigo-700">인코더 블록</h3>
                            <ul class="space-y-2 list-disc list-inside">
                                <li>Multi-Head Self-Attention</li>
                                <li>Add & Layer Normalization</li>
                                <li>Feed-Forward Network</li>
                                <li>Add & Layer Normalization</li>
                            </ul>
                        </div>
                        <div class="flex-1 grid-item">
                            <h3 class="text-purple-700">디코더 블록</h3>
                             <ul class="space-y-2 list-disc list-inside">
                                <li>Masked Multi-Head Self-Attention</li>
                                <li>Encoder-Decoder Attention</li>
                                <li>Feed-Forward Network</li>
                                <li>(각 서브레이어 후 Add & Norm 적용)</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Slide 8: Masked Attention in Decoder --><div class="slide" id="slide-7">
                <div class="slide-content w-full">
                    <h2>디코더의 Masked Self-Attention</h2>
                    <div class="grid md:grid-cols-2 gap-8 items-center">
                        <div class="text-left space-y-4">
                            <p>디코더는 다음 단어를 예측해야 하므로, 현재 예측하려는 단어 이후의 단어들을 참고해서는 안 됩니다. 이를 방지하기 위해 <b>마스킹(Masking)</b> 기법을 사용합니다.</p>
                            <p class="text-gray-700 mt-4">어텐션 점수 계산 시, 미래 위치에 해당하는 값들을 아주 작은 값(-∞)으로 설정하여 Softmax 함수를 통과할 때 그 확률이 0이 되도록 만듭니다.</p>
                        </div>
                        <div class="flex justify-center items-center">
                            <svg class="w-full max-w-sm diagram" viewBox="0 0 400 250" xmlns="http://www.w3.org/2000/svg">
                                <defs>
                                    <marker id="arrowhead-mask" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto" fill="#4a5568">
                                        <polygon points="0 0, 10 3.5, 0 7" />
                                    </marker>
                                </defs>
                                <rect x="50" y="20" width="300" height="150" rx="8" fill="#f0f2f5" stroke="#d1d5db" stroke-width="1.5"/>
                                <text x="200" y="45" text-anchor="middle" font-size="14" fill="#334155">Attention Scores</text>

                                <!-- Example Scores --><g font-family="sans-serif" font-size="10" fill="#475569">
                                    <text x="80" y="70">5.2</text><text x="130" y="70">1.8</text><text x="180" y="70">0.5</text><text x="230" y="70">0.1</text><text x="280" y="70">0.0</text><text x="330" y="70">0.0</text>
                                    <text x="80" y="90">1.0</text><text x="130" y="90">4.5</text><text x="180" y="90">0.3</text><text x="230" y="90">0.0</text><text x="280" y="90">0.0</text><text x="330" y="90">0.0</text>
                                    <text x="80" y="110">0.2</text><text x="130" y="110">0.7</text><text x="180" y="110">6.0</text><text x="230" y="110">0.0</text><text x="280" y="110">0.0</text><text x="330" y="110">0.0</text>
                                    <!-- Masked values --><text x="230" y="130" fill="#ef4444" font-weight="bold">Masked</text><text x="280" y="130" fill="#ef4444" font-weight="bold">Masked</text><text x="330" y="130" fill="#ef4444" font-weight="bold">Masked</text>
                                    <text x="230" y="150" fill="#ef4444" font-weight="bold">Masked</text><text x="280" y="150" fill="#ef4444" font-weight="bold">Masked</text><text x="330" y="150" fill="#ef4444" font-weight="bold">Masked</text>
                                </g>

                                <rect x="200" y="180" width="100" height="40" rx="5" fill="#d1fae5" stroke="#6ee7b7" stroke-width="1.5"/>
                                <text x="250" y="205" text-anchor="middle" font-size="14" fill="#334155">Softmax</text>

                                <path d="M200 170 V 180" stroke="#4a5568" stroke-width="2" marker-end="url(#arrowhead-mask)"/>
                                <path d="M250 220 V 230" stroke="#4a5568" stroke-width="2" marker-end="url(#arrowhead-mask)"/>

                                <text x="250" y="245" text-anchor="middle" font-size="14" fill="#475569">Attention Weights (Masked)</text>
                            </svg>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Slide 9: The Final Linear and Softmax Layer --><div class="slide" id="slide-8">
                <div class="slide-content w-full">
                    <h2>최종 출력 생성</h2>
                    <p class="text-left max-w-3xl mx-auto">디코더 스택의 최종 출력 벡터는 두 개의 선형 계층과 소프트맥스 함수를 거쳐 최종적으로 다음 단어에 대한 확률 분포를 생성합니다.</p>
                    <div class="diagram-container my-8">
                        <svg class="diagram" viewBox="0 0 400 300">
                            <defs><marker id="arrowhead-final" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#16a34a"/></marker></defs>
                            <g text-anchor="middle" font-family="sans-serif" font-size="14" fill="#343a40">
                                <text x="200" y="40">Decoder Output Vector</text>
                                <rect x="150" y="50" width="100" height="40" rx="5" fill="#e0f2fe"/>
                                <path d="M200 90 v 20" stroke="#4a5568" stroke-width="2" marker-end="url(#arrowhead-final)"/>
                                <rect x="150" y="110" width="100" height="40" rx="5" fill="#dbeafe"/>
                                <text x="200" y="135">Linear Layer</text>
                                <path d="M200 150 v 20" stroke="#4a5568" stroke-width="2" marker-end="url(#arrowhead-final)"/>
                                <rect x="150" y="170" width="100" height="40" rx="5" fill="#d1fae5"/>
                                <text x="200" y="195">Softmax Layer</text>
                                <path d="M200 210 v 20" stroke="#4a5568" stroke-width="2" marker-end="url(#arrowhead-final)"/>
                                <text x="200" y="245">Output Probabilities</text>
                                <path d="M160 255 h80 M160 265 h80 M160 275 h80" stroke="#a78bfa" stroke-width="2"/>
                            </g>
                        </svg>
                    </div>
                </div>
            </div>

            <!-- Slide 10: Conclusion --><div class="slide" id="slide-9">
                <div class="slide-content">
                    <h2>결론: 트랜스포머의 혁신</h2>
                    <div class="text-left space-y-4 mt-8 max-w-2xl mx-auto">
                        <div class="flex items-start space-x-4">
                            <span class="text-green-500 text-2xl mt-1">✓</span>
                            <p><b>병렬 처리 극대화:</b> RNN의 순차적 계산 한계를 극복하고, GPU를 활용한 병렬 처리를 통해 학습 속도를 크게 향상시켰습니다.</p>
                        </div>
                        <div class="flex items-start space-x-4">
                            <span class="text-green-500 text-2xl mt-1">✓</span>
                            <p><b>장거리 의존성 해결:</b> 셀프 어텐션을 통해 문장 내 멀리 떨어진 단어 간의 관계도 효과적으로 학습할 수 있습니다.</p>
                        </div>
                        <div class="flex items-start space-x-4">
                            <span class="text-green-500 text-2xl mt-1">✓</span>
                            <p><b>새로운 표준 제시:</b> 트랜스포머는 자연어 처리 분야의 새로운 표준이 되었으며, BERT, GPT 등 현대의 거의 모든 대규모 언어 모델(LLM)의 기반이 되었습니다.</p>
                        </div>
                    </div>
                </div>
            </div>

        </div>

        <div class="navigation">
            <div class="progress-bar-container">
                <div id="progressBar" class="progress-bar"></div>
            </div>
            <div class="flex justify-between items-center">
                <button id="prevBtn" class="nav-button">이전</button>
                <span id="slide-counter" class="text-sm text-gray-600"></span>
                <button id="nextBtn" class="nav-button">다음</button>
            </div>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const slides = document.querySelectorAll('.slide');
            const progressBar = document.getElementById('progressBar');
            const slideCounter = document.getElementById('slide-counter');
            const prevBtn = document.getElementById('prevBtn');
            const nextBtn = document.getElementById('nextBtn');
            let currentSlide = 0;
            const totalSlides = slides.length;

            function showSlide(index) {
                slides.forEach((slide, i) => {
                    slide.classList.toggle('active', i === index);
                });
                updateNavigation();
            }

            function updateNavigation() {
                prevBtn.disabled = currentSlide === 0;
                nextBtn.disabled = currentSlide === totalSlides - 1;
                const progressPercentage = ((currentSlide) / (totalSlides - 1)) * 100;
                progressBar.style.width = `${progressPercentage}%`;
                slideCounter.textContent = `${currentSlide + 1} / ${totalSlides}`;
            }

            nextBtn.addEventListener('click', () => {
                if (currentSlide < totalSlides - 1) {
                    currentSlide++;
                    showSlide(currentSlide);
                }
            });

            prevBtn.addEventListener('click', () => {
                if (currentSlide > 0) {
                    currentSlide--;
                    showSlide(currentSlide);
                }
            });

            // Keyboard navigation
            document.addEventListener('keydown', (e) => {
                if (e.key === 'ArrowRight' && currentSlide < totalSlides - 1) {
                    nextBtn.click();
                } else if (e.key === 'ArrowLeft' && currentSlide > 0) {
                    prevBtn.click();
                }
            });
            
            showSlide(currentSlide);
        });
    </script>
</body>
</html>

