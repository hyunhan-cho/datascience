// BERT Fine-Tuning Tutorial - Typst Document
// ì €ì¥ í›„ `typst compile bert-tutorial.typ` ë¡œ PDF ìƒì„±

#set document(
  title: "BERT Fine-Tuning Lab",
  author: "Machine Learning Study",
)

#set page(
  paper: "a4",
  margin: (x: 2cm, y: 2.5cm),
  header: align(right)[
    #text(size: 9pt, fill: gray)[BERT Fine-Tuning Lab]
  ],
  footer: context align(center)[
    #counter(page).display("1 / 1", both: true)
  ],
)

#set text(
  font: ("Pretendard", "Noto Sans KR", "Inter"),
  size: 11pt,
  lang: "ko",
)

#set heading(numbering: "1.")

#show heading.where(level: 1): it => block(
  fill: rgb("#4f46e5"),
  inset: (x: 12pt, y: 10pt),
  radius: 6pt,
  width: 100%,
  text(fill: white, weight: "bold", size: 14pt)[#it.body],
)

#show heading.where(level: 2): it => block(
  above: 1.5em,
  below: 0.8em,
  text(fill: rgb("#4f46e5"), weight: "bold", size: 12pt)[#it.body],
)

// ì½”ë“œ ë¸”ë¡ ìŠ¤íƒ€ì¼
#show raw.where(block: true): it => block(
  fill: rgb("#1f2937"),
  inset: 12pt,
  radius: 8pt,
  width: 100%,
  text(fill: rgb("#e5e7eb"), size: 9pt, font: "JetBrains Mono")[#it],
)

// ì¸ë¼ì¸ ì½”ë“œ ìŠ¤íƒ€ì¼
#show raw.where(block: false): it => box(
  fill: rgb("#f3f4f6"),
  inset: (x: 4pt, y: 2pt),
  radius: 3pt,
  text(fill: rgb("#4f46e5"), size: 10pt, font: "JetBrains Mono")[#it],
)

// ì»¤ìŠ¤í…€ ì»´í¬ë„ŒíŠ¸
#let note(title: none, body) = block(
  fill: rgb("#eff6ff"),
  stroke: (left: 4pt + rgb("#3b82f6")),
  inset: 12pt,
  radius: (right: 6pt),
  width: 100%,
)[
  #if title != none [
    #text(weight: "bold", fill: rgb("#1e40af"))[#title] \
  ]
  #text(size: 10pt)[#body]
]

#let output(body) = block(
  fill: rgb("#f9fafb"),
  stroke: 1pt + rgb("#e5e7eb"),
  inset: 12pt,
  radius: 6pt,
  width: 100%,
)[
  #text(size: 8pt, fill: gray)[â–¸ Output] \
  #text(font: "JetBrains Mono", size: 9pt)[#body]
]

#let tip(body) = block(
  fill: rgb("#fef9c3"),
  stroke: 1pt + rgb("#fcd34d"),
  inset: 10pt,
  radius: 6pt,
  width: 100%,
)[
  #text(weight: "bold", fill: rgb("#92400e"))[ğŸ’¡ Tip:] #body
]

// ========== ë¬¸ì„œ ì‹œì‘ ==========

#align(center)[
  #v(2cm)
  #text(size: 32pt, weight: "bold", fill: rgb("#4f46e5"))[ğŸ§  BERT Fine-Tuning Lab]
  #v(0.5cm)
  #text(size: 14pt, fill: gray)[DistilBERTë¡œ IMDB ë¦¬ë·° ê°ì • ë¶„ë¥˜í•˜ê¸°]
  #v(0.3cm)
  #line(length: 60%, stroke: 1pt + rgb("#e5e7eb"))
  #v(0.5cm)
  #text(size: 10pt, fill: gray)[Machine Learning Study Â· 2025]
  #v(2cm)
]

= ì‹¤ìŠµ ì†Œê°œ

ì´ íŠœí† ë¦¬ì–¼ì€ *IMDB ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹*ì„ ì‚¬ìš©í•˜ì—¬ ê¸ì •/ë¶€ì •ì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëœ `distilbert-base-uncased` ëª¨ë¸ì„ ê°€ì ¸ì™€ ìš°ë¦¬ì˜ ëª©ì ì— ë§ê²Œ *íŒŒì¸íŠœë‹(Fine-tuning)* í•©ë‹ˆë‹¤.

#grid(
  columns: (1fr, 1fr),
  gutter: 16pt,
  block(
    fill: rgb("#f8fafc"),
    stroke: 1pt + rgb("#e2e8f0"),
    inset: 14pt,
    radius: 8pt,
  )[
    #text(fill: rgb("#4f46e5"), weight: "bold")[STEP 1 Â· ë¬¸ì œ ì •ì˜] \
    #text(size: 10pt)[ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ê¸ì •(1) ë˜ëŠ” ë¶€ì •(0)ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.]
  ],
  block(
    fill: rgb("#f8fafc"),
    stroke: 1pt + rgb("#e2e8f0"),
    inset: 14pt,
    radius: 8pt,
  )[
    #text(fill: rgb("#7c3aed"), weight: "bold")[STEP 2 Â· ëª¨ë¸ ì •ë³´] \
    #text(size: 10pt)[DistilBERT(ê²½ëŸ‰í™”ëœ BERT)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. (íŒŒë¼ë¯¸í„°: ì•½ 66M)]
  ],
)

#v(1em)

= ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

#note(title: "í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜")[
  Hugging Faceì˜ ìƒíƒœê³„ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
  - `transformers`: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(BERT ë“±)ì„ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¤ê³  í•™ìŠµì‹œí‚¤ëŠ” ë„êµ¬
  - `datasets`: IMDB ê°™ì€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ë¥¼ í•œ ì¤„ì˜ ì½”ë“œë¡œ ë‹¤ìš´ë¡œë“œ
  - `fsspec`: íŒŒì¼ ì‹œìŠ¤í…œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì˜ì¡´ì„± íŒ¨í‚¤ì§€
]

```python
# ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘ í•„ìš” (ì½”ë© í™˜ê²½ì¼ ê²½ìš°)
!pip install -q transformers datasets
!pip install -U "datasets<=2.18.0" "fsspec<=2023.6.0"
```

= ë°ì´í„° ë¡œë“œ ë° ë¶„í• 

#note(title: "IMDB ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°")[
  `load_dataset` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì´ ì‹¤ìŠµì—ì„œëŠ” ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ *50ê°œì˜ ìƒ˜í”Œ*ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

  *ì½”ë“œ ìƒì„¸ ë¶„ì„:*
  - `split="train[:50]"`: ì „ì²´ í•™ìŠµ ë°ì´í„° ì¤‘ ì•ì—ì„œ 50ê°œë§Œ ìŠ¬ë¼ì´ì‹±
  - `.train_test_split(test_size=0.2)`: 8:2 ë¹„ìœ¨ë¡œ í•™ìŠµìš©(40ê°œ)ê³¼ ê²€ì¦ìš©(10ê°œ)ìœ¼ë¡œ ë¶„ë¦¬
]

```python
from datasets import load_dataset

# 1. ë°ì´í„° ë¡œë“œ
# IMDB ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ ì¤‘ 50ê°œ ìƒ˜í”Œë§Œ ê°€ì ¸ì™€ì„œ í•™ìŠµìš©/í‰ê°€ìš©ìœ¼ë¡œ 8:2 ë¹„ìœ¨ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤
dataset = load_dataset("imdb", split="train[:50]").train_test_split(test_size=0.2)
```

== ë°ì´í„° í™•ì¸í•˜ê¸°

ë°ì´í„°ê°€ ì˜ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ 8ë²ˆì§¸ ìƒ˜í”Œ(ì¸ë±ìŠ¤ 7)ì„ ì¶œë ¥í•´ ë´…ë‹ˆë‹¤.

```python
sample = dataset["train"][7]
print(f"ğŸ“ ë¦¬ë·° ë‚´ìš©:\n{sample['text']}\n")
print(f"ğŸ·ï¸ ë¼ë²¨ (0=ë¶€ì •, 1=ê¸ì •): {sample['label']}")
```

#output[
  ğŸ“ ë¦¬ë·° ë‚´ìš©:
  I received this movie as a gift, I knew from the DVD cover... (ì¤‘ëµ) ... Even if you like B HORROR movies, don't watch this movie

  ğŸ·ï¸ ë¼ë²¨ (0=ë¶€ì •, 1=ê¸ì •): 0
]

#text(
  size: 9pt,
  fill: gray,
  style: "italic",
)[ë¼ë²¨ 0ì€ ë¶€ì •ì ì¸ ë¦¬ë·°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë¦¬ë·° ë‚´ìš© ë§ˆì§€ë§‰ì˜ 'don\'t watch this movie'ì—ì„œ ë¶€ì •ì ì¸ ê°ì •ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.]

= ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì¤€ë¹„

#note(title: "ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°")[
  ìš°ë¦¬ëŠ” ì²˜ìŒë¶€í„° ì˜ì–´ë¥¼ ê°€ë¥´ì¹˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì´ë¯¸ ì˜ì–´ë¥¼ ì˜í•˜ëŠ” *DistilBERT* ëª¨ë¸ì„ ë°ë ¤ì™€ì„œ "ë¦¬ë·° ë¶„ë¥˜" ì—…ë¬´ë¥¼ ê°€ë¥´ì¹  ê²ƒì…ë‹ˆë‹¤.
]

#grid(
  columns: (1fr, 1fr),
  gutter: 12pt,
  block(fill: rgb("#eef2ff"), inset: 10pt, radius: 6pt)[
    *Tokenizer* \
    #text(size: 9pt)[ì‚¬ëŒì˜ ì–¸ì–´(ê¸€ì)ë¥¼ ëª¨ë¸ì´ ì´í•´í•˜ëŠ” ì–¸ì–´(ìˆ«ì)ë¡œ ë²ˆì—­í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.]
  ],
  block(fill: rgb("#eef2ff"), inset: 10pt, radius: 6pt)[
    *Model* \
    #text(size: 9pt)[ìˆ«ìë¡œ ëœ ì–¸ì–´ë¥¼ ì…ë ¥ë°›ì•„ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë‘ë‡Œì…ë‹ˆë‹¤.]
  ],
)

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì¤€ë¹„
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
# uncased: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ëª¨ë‘ ì†Œë¬¸ìë¡œ ì²˜ë¦¬í•œë‹¤ëŠ” ì˜ë¯¸

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
# SequenceClassification: ë¬¸ì¥ ë¶„ë¥˜ë¥¼ ìœ„í•œ í—¤ë“œ(Head)ê°€ ë‹¬ë¦° ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´
```

== ë°ì´í„° ì „ì²˜ë¦¬ (í† í°í™”)

#note(title: "ì „ì²˜ë¦¬ ì˜µì…˜ ì„¤ëª…")[
  - `truncation=True`: ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ ëª¨ë¸ì˜ ìµœëŒ€ ê¸¸ì´(512)ì— ë§ì¶° ìë¦…ë‹ˆë‹¤.
  - `padding=True`: ë¬¸ì¥ì´ ì§§ìœ¼ë©´ ë¹ˆ ê³µê°„ì„ 0ìœ¼ë¡œ ì±„ì›Œ ê¸¸ì´ë¥¼ ë§ì¶¥ë‹ˆë‹¤.
  - `batched=True`: í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì”© ì²˜ë¦¬í•˜ì—¬ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
]

```python
# 3. ë°ì´í„° ì „ì²˜ë¦¬ (í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì í† í°ìœ¼ë¡œ ë³€í™˜)
def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, padding=True)

dataset = dataset.map(tokenize, batched=True)
```

= í›ˆë ¨ ì„¤ì •

#note(title: "Trainer ì„¤ì • ë° ë©”íŠ¸ë¦­ í•¨ìˆ˜")[
  Hugging Faceì˜ `Trainer` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ ë³µì¡í•œ í•™ìŠµ ë£¨í”„(forë¬¸)ë¥¼ ì§ì ‘ ì§¤ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì— í•„ìš”í•œ ì„¤ì •ê°’(Hyperparameters)ê³¼ í‰ê°€ ë°©ë²•ë§Œ ì •í•´ì£¼ë©´ ë©ë‹ˆë‹¤.
]

```python
from transformers import TrainingArguments, Trainer
from sklearn.metrics import accuracy_score

# ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜: ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ì •ë‹µì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)  # í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í´ë˜ìŠ¤ ì„ íƒ
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc}

# í›ˆë ¨ íŒŒë¼ë¯¸í„° ì„¤ì •
args = TrainingArguments(
    output_dir="test",              # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
    per_device_train_batch_size=8,  # ë°°ì¹˜ í¬ê¸° (í•œ ë²ˆì— í•™ìŠµí•  ë°ì´í„° ìˆ˜)
    num_train_epochs=15,            # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ (ì „ì²´ ë°ì´í„°ë¥¼ 15ë²ˆ ë´„)
    report_to="none",               # ì™¸ë¶€ ë¡œê¹… ë¹„í™œì„±í™”
    logging_steps=1                 # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
)

# Trainer ê°ì²´ ìƒì„±
trainer = Trainer(
    model=model,                    # í•™ìŠµì‹œí‚¬ ëª¨ë¸
    args=args,                      # ìœ„ì—ì„œ ì •ì˜í•œ ì„¤ì •ê°’
    train_dataset=dataset["train"], # í•™ìŠµ ë°ì´í„° (40ê°œ)
    eval_dataset=dataset["test"],   # í‰ê°€ ë°ì´í„° (10ê°œ)
    compute_metrics=compute_metrics # í‰ê°€ ë°©ì‹ (ì •í™•ë„)
)
```

= íŒŒì¸íŠœë‹ ì‹¤í–‰

ì´ì œ ëª¨ë“  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤. `trainer.train()` í•œ ì¤„ì´ë©´ í•™ìŠµì´ ì‹œì‘ë©ë‹ˆë‹¤. ëª¨ë¸ì€ ìš°ë¦¬ê°€ ì œê³µí•œ 40ê°œì˜ ë°ì´í„°ë¥¼ 15ë²ˆ ë°˜ë³µí•´ì„œ ë³´ë©°(Epochs), ê¸ì •ê³¼ ë¶€ì •ì„ êµ¬ë¶„í•˜ëŠ” ë²•ì„ ìŠ¤ìŠ¤ë¡œ ê¹¨ìš°ì¹©ë‹ˆë‹¤.

```python
# 5. íŒŒì¸íŠœë‹ ì‹¤í–‰ (ëª¨ë¸ í•™ìŠµ)
trainer.train()
```

#output[
  [75/75 00:34, Epoch 15/15]
  Step    Training Loss
  1       0.774700
  ...
  75      0.001400

  TrainOutput(global_step=75, training_loss=0.0416, ...)
]

#text(
  size: 10pt,
)[â†’ *Training Loss*ê°€ 0.77ì—ì„œ ì‹œì‘í•´ 0.0014ê¹Œì§€ ë–¨ì–´ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ê±°ì˜ ì™„ë²½í•˜ê²Œ ì ì‘í–ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.]

== ëª¨ë¸ í‰ê°€

í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•Šì€ *í…ŒìŠ¤íŠ¸ ë°ì´í„°(10ê°œ)*ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì˜ ì‹¤ì œ ì‹¤ë ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.

```python
results = trainer.evaluate()
print(results)
```

#output[
  {'eval_loss': 0.00109, 'eval_accuracy': 1.0, ...}
]

#text(fill: rgb("#16a34a"), weight: "bold")[âœ“ ì •í™•ë„(accuracy)ê°€ 1.0(100%)ì´ ë‚˜ì™”ìŠµë‹ˆë‹¤!] #text(
  size: 9pt,
  fill: gray,
)[(ìƒ˜í”Œ ë°ì´í„°ê°€ ì ì–´ì„œ ë‚˜ì˜¨ ê²°ê³¼ì…ë‹ˆë‹¤)]

= ì‹¤ì œ ì˜ˆì¸¡ ìˆ˜í–‰

#note(title: "ìƒˆë¡œìš´ ë¬¸ì¥ í…ŒìŠ¤íŠ¸")[
  ì´ì œ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´, ë³¸ ì  ì—†ëŠ” ìƒˆë¡œìš´ ë¦¬ë·° ë¬¸ì¥ì„ íŒë³„í•´ ë´…ë‹ˆë‹¤. ì´ ê³¼ì •ì„ *ì¸í¼ëŸ°ìŠ¤(Inference)*ë¼ê³  í•©ë‹ˆë‹¤.
]

#tip[ëª¨ë¸ì€ GPU(cuda)ì— ìˆìœ¼ë¯€ë¡œ, ì…ë ¥ ë°ì´í„°ë„ `.to("cuda")`ë¥¼ í†µí•´ GPUë¡œ ì˜®ê²¨ì¤˜ì•¼ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤.]

```python
# 6. í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹¤ì œ ì˜ˆì¸¡ ìˆ˜í–‰
text = "I would put this at the top of my list of films in the category of unwatchable trash!"
# "ì´ ì˜í™”ë¥¼ ë‚´ 'ì‹œì²­ ë¶ˆê°€ ì“°ë ˆê¸°' ì¹´í…Œê³ ë¦¬ ì˜í™” ë¦¬ìŠ¤íŠ¸ì˜ ë§¨ ìœ„ì— ì˜¬ë¦¬ê² ë‹¤!" (í˜¹í‰)

# 1. í† í°í™” ë° GPUë¡œ ì´ë™
inputs = tokenizer(text, return_tensors="pt").to("cuda")

# 2. ëª¨ë¸ ì˜ˆì¸¡
output = model(**inputs)

# 3. ê²°ê³¼ í•´ì„ (Logits -> Label)
label = output.logits.argmax(-1).item()
# output.logitsëŠ” [ë¶€ì •ì ìˆ˜, ê¸ì •ì ìˆ˜] í˜•íƒœì…ë‹ˆë‹¤.
# argmax(-1)ì€ ë‘˜ ì¤‘ ë” í° ì ìˆ˜ì˜ ìœ„ì¹˜(ì¸ë±ìŠ¤)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

print("ê¸ì •" if label == 1 else "ë¶€ì •")
```

#output[
  ë¶€ì •
]

#v(1.5em)

#block(
  fill: gradient.linear(rgb("#4f46e5"), rgb("#7c3aed"), angle: 45deg),
  inset: 20pt,
  radius: 12pt,
  width: 100%,
)[
  #align(center)[
    #text(fill: white, size: 18pt, weight: "bold")[ğŸ‰ ì‹¤ìŠµ ì™„ë£Œ!]
    #v(0.5em)
    #text(fill: rgb("#e0e7ff"), size: 11pt)[
      ì´ì œ ì—¬ëŸ¬ë¶„ì€ *BERT ëª¨ë¸ì„ íŒŒì¸íŠœë‹*í•˜ì—¬ \
      ê°ì • ë¶„ì„ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
    ]
  ]
]
